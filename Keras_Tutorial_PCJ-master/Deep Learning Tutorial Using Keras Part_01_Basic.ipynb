{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep Learning Tutorial Using Keras Part01 - Basic.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7BK95rFutQ3Q","colab_type":"text"},"source":["제목: Deep Learning Tutorial Using Keras Part01 - Basic (Keras로 배우는 딥러닝 튜토리얼 Part01 기초편) <br>\n","제작자: Park Chanjun (박찬준) <br>\n","소속: Korea University Natural Language Processing & Artificial Intelligence Lab (고려대학교 자연언어처리&인공지능 연구실)<br>\n","Email: bcj1210@naver.com<br>\n","참고자료: 케라스 창시자에게 배우는 딥러닝\n"]},{"cell_type":"markdown","metadata":{"id":"YpT34NjUt-gu","colab_type":"text"},"source":["![인공지능과 기계학습 그리고 딥러닝](https://t1.daumcdn.net/thumb/R720x0/?fname=http://t1.daumcdn.net/brunch/service/user/7Wih/image/WvebYXc34fDKzbmNyrvUjQ_6Umc.png)"]},{"cell_type":"markdown","metadata":{"id":"JSjL2i1nuaWa","colab_type":"text"},"source":["**인공지능**<br>\n","보통의 사람이 수행하는 지능적인 작업을 자동화하기 위한 연구활동<br><br>\n","**심볼릭 AI(1950~1980)**<br>\n","명시적인 규칙을 충분하게 많이 만들어 지식을 다루면 인간 수준의 인공지능을 만들 수 있을 것이라는 패러다임<br>\n","대표적으로 전문가 시스템이 있다. <br>\n","이미지 분류, 기계번역 같은 불분명한 문제를 해결하기 위한 명확한 규칙을 찾는 것은 어렵다는 단점이 있다.<br><br>\n","**머신러닝**<br>\n","프로그래머가 직접 만든 데이터처리 규칙 대신 컴퓨터가 데이터를 보고 자동으로 이러한 규칙을 학습할 수 있을까? 라는 질문에서 시작. <br>\n","\n","기존) 규칙 + 데이터 = 해답 <br>\n","머신러닝) 데이터 + 해답 = 규칙<br>\n","\n","머신러닝을 위해 필요한 것\n","1. 입력 데이터 \n","2. 기대 출력 (즉 정답)\n","3. 알고리즘의 성능을 측정하는 방법 = 알고리즘의 현재 출력과 기대 출력 간의 차이를 결정하기 위해 필요\n","\n","결론적으로 입력데이터를 기반으로 기대출력에 가깝게 만드는 유용한 표현을 학습 하는 것이다.\n","즉 학습이란 더 나은 표현을 찾는 자동화된 과정이다.\n","\n","즉 머신러닝은 가능성 있는 공간을 사전에 정의하고 피드백 신호의 도움을 받아 입력 데이터에 대한 유용한 변환을 찾는 것이다.<br><br>\n","\n","**딥러닝**<br>\n","딥러닝은 머신러닝의 한 분야로 연속된 층(Layer)에서 점진적으로 의미 있는 표현을 배우는 것.<br>\n","딥이란 결국 연속된 층으로 표현을 학습한다는 개념이다.<br>\n","즉 Layer Representations Learning, Hierarchical Representations Learning이다.<br>\n","![딥러닝](https://t1.daumcdn.net/news/201805/02/HMGjournal/201805020955461105cjq.png) <br>\n","\n","즉 딥러닝은 정보가 연속된 필터를 통과 하면서 순도 높게 정제되는 다단계 정보 추출 작업으로 생각할 수 있다. <br>\n","즉 데이터 표현을 학습하기 위한 다단계 처리방식을 말한다.<br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xr5blk_Fwb0G","colab_type":"text"},"source":["**학습(Learning)**<br>\n","\n","층에서 입력 데이터가 처리되는 상세 내용은 일련의 숫자로 이루어진 층의 가중치(Weight)에 저장되어 있다. <br>어떤 층에서 일어나는 변환은 그 층의 가중치를 파라미터로 가지는 함수로 표현 되는 것이다. <br>\n","\n","학습이란 주어진 입력을 정확한 타깃에 매칭하기 위해 신경망의 모든 층에 있는 가중치 값을 찾는 것을 의미한다.<br>\n","즉 학습이란 최적의 Weight 파라미터를 찾는 것이라 한마디로 정의해볼 수 있다.<br>\n","\n","**손실함수(Loss Function)**<br>\n","최적을 찾으려면 어떤 것을 계속 조정해보면서 찾아야 한다!<br>\n","즉 신경망의 출력을 제어하려면 출력이 기대하는 것보다 얼마나 벗어나는지를 먼저 측정해야한다.<br>\n","이는 신경망의 손실 함수(Loss Function) 또는 목적함수(Objective Function)이 담당한다.<br>\n","신경망이 한 샘플에 대해 얼마나 잘 예측했는지 측정하기 위해 손실함수가 신경망의 예측과 진짜 타깃의 차이를 점수로 계산한다! <br>\n","\n","**역전파와 옵티마이저(BackPropagation and Optimizer)**<br>\n","즉 기본적인 딥러닝 방식은 이 점수를 현재 샘플의 손실 점수가 감소되는 방향으로 가중치 값을 조금씩 수정하는 것이다. <br>\n","이 작업을 딥러닝의 역전파(Backpropagation)을 구현한 옵티마이저(Optimizer)가 담당한다.<br>\n","![학습과정](http://circlehaus.s3.dualstack.ap-northeast-2.amazonaws.com/original/1X/fff918f1cb5933e3971bf68edc12659b6a5a1851.png)"]},{"cell_type":"markdown","metadata":{"id":"tSLct1iazsMP","colab_type":"text"},"source":["**한눈에 보는 딥러닝의 성과들**<br>\n","![대체 텍스트](https://t1.daumcdn.net/cfile/tistory/25417A4257E932AC25)"]},{"cell_type":"markdown","metadata":{"id":"U_8uGWf91NPT","colab_type":"text"},"source":["**Why Deep Learning**\n","1. 하드웨어의 발전(GPU)\n","2. 대용량 데이터셋\n","3. 알고리즘의 향상\n","- 신경망 층에 더 잘 맞는 활성화 함수(Activation Function)\n","- 층별 사전 훈련(Pretraining)을 불필요하게 만든 가중치 초기화 (Weight initialization) 방법\n","- Adam 및 Radam과 같은 좋은 최적화 방법들.\n","4. 오픈소스"]},{"cell_type":"markdown","metadata":{"id":"L6b-t4RK2I8G","colab_type":"text"},"source":["이제 부터 Keras코드로 딥러닝을 배워보도록 하겠습니다.\n","기초부터 차근차근 !\n","\n","먼저 Mnist예제로 신경망의 기초를 배워보도록 하겠습니다."]},{"cell_type":"code","metadata":{"id":"kS8WOkpRtP5y","colab_type":"code","colab":{}},"source":["from keras.datasets import mnist\n","\n","(train_images,train_labels),(test_images,test_labels)= mnist.load_data() #데이터 로드하기 , 학습데이터와 테스트 데이터\n","\n","#먼저 모양을 한번 보도록 하겠습니다.\n","print(\"Shape: \",train_images.shape)\n","print(\"길이: \",len(train_images))\n","print(\"Shape: \",test_images.shape)\n","print(\"길이: \",len(test_images))\n","\n","#총 6만개의 학습데이터,1만개의 테스트 데이터 28*28(픽셀)임을 볼 수 있습니다."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UTnHMMNb3rwY","colab_type":"text"},"source":["데이터 정보를 알았으니 이제 모델을 만들어 보도록 하겠습니다.<br>\n","\n","신경망의 핵심 구성요소는 바로 층(Layer) 입니다.<br>\n","층은 주어진 문제에 더 의미있는 표현을 입력된 데이터로부터 추출합니다.<br>\n","\n","아래의 예시는 Fully Connected된 신경망 층이 2개가 연소되어 있으며 2번째 층은 10개의 확률 점수가 들어 있는 배열을 반환하는 소프트맥스(Softmax) 층입니다.<br>\n","10개인 이유는 MNIST가 숫자 0~9까지 구분하는 예제이기 때문입니다."]},{"cell_type":"code","metadata":{"id":"K461DDd1tPRK","colab_type":"code","colab":{}},"source":["from keras import models\n","from keras import layers\n","\n","model=models.Sequential()\n","model.add(layers.Dense(512,activation=\"relu\",input_shape=(28*28,)))\n","model.add(layers.Dense(10,activation=\"softmax\"))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4aQyHZB4u-X","colab_type":"text"},"source":["신경망이 훈련 준비를 마치기 위해서 컴파일 단계에 총 3가지 정보가 필요합니다.\n","\n","1. 손실함수(Loss Function) : 훈련 데이터에서 신경망의 성능을 측정하는 방법으로 네트워크가 옳은 방향으로 학습될 수 있도록 도와준다.\n","\n","2. 옵티마이저(Optimizer): 입력된 데이터와 손실함수를 기반으로 네트워크를 업데이트하는 매커니즘\n","\n","3. 훈련과 테스트 과정을 모니터링할 지표: 정확도를 의미!"]},{"cell_type":"code","metadata":{"id":"uPZonEjD5Kl0","colab_type":"code","colab":{}},"source":["model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h1-FhzjS5W02","colab_type":"text"},"source":["훈련을 시작하기 전 !!!!<br>\n","데이터를 네트워크에 맞는 크기로 바꾸고 모든 갑을 0과 1 사이로 스케일을 조정해보겠습니다.\n","\n","또한 레이블을 범주형으로 변경해보겠습니다 . (one hot)\n"]},{"cell_type":"code","metadata":{"id":"JqANl9Fg5fWr","colab_type":"code","colab":{}},"source":["train_images=train_images.reshape((60000,28*28))\n","train_images=train_images.astype('float32')/255\n","\n","test_images=test_images.reshape((10000,28*28))\n","test_images=test_images.astype('float32')/255\n","\n","from keras.utils import to_categorical\n","\n","train_labels=to_categorical(train_labels)\n","test_labels=to_categorical(test_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xiiaQys-6HGn","colab_type":"text"},"source":["이제 훈련을 실제 해보도록 하겠습니다.<br>\n","keras에서는 fit 함수를 이용해 훈련을 진행합니다."]},{"cell_type":"code","metadata":{"id":"NLY9i_op6NDG","colab_type":"code","colab":{}},"source":["model.fit(train_images,train_labels,epochs=5,batch_size=128)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hePK7_976tF2","colab_type":"text"},"source":["이제 성능을 측정해보도록 하겠습니다.\n"]},{"cell_type":"code","metadata":{"id":"2B5x_91b6vzT","colab_type":"code","colab":{}},"source":["test_loss,test_acc=model.evaluate(test_images,test_labels)\n","print(\"정확도: \",test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yi6-6_h-7XYR","colab_type":"text"},"source":["이제 모든 코드를 한눈에 보도록 하겠습니다."]},{"cell_type":"code","metadata":{"id":"XIVflgVH7aE8","colab_type":"code","colab":{}},"source":["from keras.datasets import mnist\n","from keras import models\n","from keras import layers\n","from keras.utils import to_categorical\n","\n","#데이터 로드하기 , 학습데이터와 테스트 데이터\n","(train_images,train_labels),(test_images,test_labels)= mnist.load_data() \n","\n","#데이터 전처리\n","train_images=train_images.reshape((60000,28*28))\n","train_images=train_images.astype('float32')/255\n","\n","test_images=test_images.reshape((10000,28*28))\n","test_images=test_images.astype('float32')/255\n","\n","train_labels=to_categorical(train_labels)\n","test_labels=to_categorical(test_labels)\n","\n","#모델 설계\n","model=models.Sequential()\n","model.add(layers.Dense(512,activation=\"relu\",input_shape=(28*28,)))\n","model.add(layers.Dense(10,activation=\"softmax\"))\n","\n","#모델 컴파일\n","model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","#모델 훈련\n","model.fit(train_images,train_labels,epochs=5,batch_size=128)\n","\n","#정확도 측정\n","test_loss,test_acc=model.evaluate(test_images,test_labels)\n","print(\"정확도: \",test_acc)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V1H_sMMl8B1-","colab_type":"text"},"source":["**텐서**<br>\n","텐서는 데이터를 위한 Container이다.<br>\n","텐서에서 차원을 종종 축(Axis)라고 부르기도 한다.\n","텐서의 축 개수: 랭크(Rank)\n","\n","**중요 개념**\n","1. 축의 개수(랭크): 예를 들어 3D 텐서에서는 3개의 축이 있고, 행렬에는 2개의 축이 있는 것. <br>넘파이의 ndim 속성에 저장되어 있는 값.\n","\n","2. 크기(Shape): 텐서의 각 축을 따라 얼마나 많은 차원이 있는지를 나타낸 파이썬의 튜플 값.<br>벡터의 크기는 (5,)처럼 1개의 원소로 이루어진 튜플 !, 스칼라는 크기가 없음.\n","\n","3. 데이터 타입: 넘파이의 dtype에 저장된 값. 텐서에 포함된 데이터의 타입이다.\n","\n","\n","\n","**스칼라(0D 텐서)**<br>\n","하나의 숫자만 담고 있는 텐서를 스칼라라고 한다.<br>\n","스칼라 텐서, 0차원 텐서, 0D텐서라고 한다.<br>\n","\n","**벡터(1D 텐서)**<br>\n","숫자의 배열<br>\n","\n","**행렬(2D 텐서)**<br>\n","행렬은 2D 텐서이다.<br>\n","행(row)과 열(column)으로 이루어짐<br>\n","\n","**3D텐서와 고차원 텐서**<br>\n","3D 텐서들을 하나의 배열로 합치면 4D 텐서를 만드는 식으로 이루어짐 !!<br>\n","딥러닝에서는 보통 0D~4D까지의 텐서를 다룬다.<br>\n","동영상의 경우 5D까지 텐서를 다루기도 한다.<br>\n","\n","**텐서의 실제 사례**\n","1. 벡터 데이터: (sample,features)크기의 2D 텐서\n","2. 시계열 데이터 또는 시퀀스 데이터: (sample,timesteps,features) 크기의 텐서\n","3. 이미지: (samples, height,width,channels) 또는 (samples,channels, height,width) 크기의 3D 텐서 (채널 마지막(channel-last) 방식과 (channel-first) 방식)\n","4. 동영상 (samples,frames,height,width,channels) 또는 (samples,frames,channels,height,width) 크기의 5D 텐서 \n","\n","**Transpose(전치)**<br>\n","행렬의 전치란 행과 열을 바꾸는 것을 의미한다.\n"]},{"cell_type":"code","metadata":{"id":"klxX16kY_B0v","colab_type":"code","colab":{}},"source":["#넘파이로 텐서 조작해보기\n","from keras.datasets import mnist\n","\n","(train_images,train_labels),(test_images,test_labels)= mnist.load_data() \n","print(train_images.shape)\n","\n","slice_data=train_images[10:100]#11번째 데이터부터 101번째 전까지 !!, 즉 (90,28,28)\n","print(slice_data.shape)\n","\n","slice_data=train_images[10:100,:,:]#더 자세한 표기법 !! \":\" 는 전체를 의미한다.\n","print(slice_data.shape)\n","\n","slice_data=train_images[10:100,0:28,0:28]#더더 자세한 표기법 \n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAoukQo8ADmj","colab_type":"text"},"source":["일반적으로 딥러닝에서 사용하는 모든 데이터 텐서의 첫번째 축(0번째 축)은 샘플 축(sample axis)이다. 샘플 차원(sample dimension)이라고 부르기도 한다.\n","\n","왜 이 개념을 말하는가?\n","\n","바로 딥러닝 모델은 한번에 전체 데이터 셋 전체를 처리하지 않기 때문이다.\n","작은 batch(배치)로 나누어 학습하게 된다.<br>\n","이런 배치 데이터를 다룰 때는 첫번째 축을 배치 축(batch axis) 또는 배치 차원(batch dimension)이라고 부른다."]},{"cell_type":"code","metadata":{"id":"4ukRafk_AeGF","colab_type":"code","colab":{}},"source":["batch=train_images[:128]\n","batch=train_images[128:256]\n","\n","batch=train_images[128*n:128*(n+1)]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3j2f64RJG2VJ","colab_type":"text"},"source":["훈련에 대해서 더 자세히 알아보자\n","output=relu(dot(W,input)*b)\n","\n","W는 가중치 또는 훈련되는 파라미터(trainable parameter)라고 부른다.\n","초기에는 가중치 행렬이 작은 난수로 채워져 있다. 이는 무작위 초기화(random initialization)단계라고 부른다.\n","즉 의미없는 표현이 만들어지며 이 표현을 점진적으로 조정하면서 훈련을 진행하는 것이다.\n","\n","1. 훈련 샘플 x와 이에 상응하는 정답 y의 배치를 추출한다.\n","2. x를 사용하여 네트워크를 실행하고(forward) , 예측 값 y_pred를 구한다.\n","3. y_pred와 y의 차이를 측정하여 이 배치에 대한 네트워크 손실을 계산한다.즉 손실함수의 그레디언트 계산(backward)\n","4. 배치에 대한 손실이 조금 감소되도록 네트워크의 모든 가중치를 업데이트 한다. 즉 그레디언트의 반대 방향으로 파라미터를 조금 씩 이동시킨다. W= W- step*gradient\n","5. 이렇게 되면 결국 훈련 데이터에서 네트워크의 손실, 즉 예측 y_ped와 정답 y의 오차가 매우 작아질 것이다.\n","\n","![대체 텍스트](https://user-images.githubusercontent.com/11629647/52923259-42b1a880-336a-11e9-8e7c-b3e6cec12d5c.png)\n","\n","![옵티마이저](https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848)\n","\n","![대체 텍스트](https://miro.medium.com/max/2598/1*1QByL6t7uDvij1EkNyvDrw.png)"]},{"cell_type":"markdown","metadata":{"id":"VNWqMpnSHa_a","colab_type":"text"},"source":["**본격적인 Keras 실습 및 신경망 시작하기**<br>\n","\n","keras 자체에 대한 특징 및 자세한 설명은 구글링을 해주세요.\n","\n","1. 입력 텐서와 타겟 텐서로 이루어진 훈련 데이터를 정의한다.\n","2. 입력과 타겟을 매핑하는 층으로 이루어진 모델(혹은 네트워크)를 정의한다.\n","3. 손실함수, 옵티마이저, 모니터링을 하기 위한 측정 지표를 선택하여 학습과정을 설정한다.\n","4. 훈련 데이터에 대해 모델의 fit() 메서드를 반복적으로 호출하여 훈련을 시작한다.\n","\n","<br>\n","Keras의 코딩 방법은 크게 Sequential 클래스를 이용하거나 함수형 API를 사용하는 방법이 있다. (모델 구조 정의에 한함)"]},{"cell_type":"code","metadata":{"id":"2rjXk1wDKKHH","colab_type":"code","colab":{}},"source":["#Sequential 클래스 이용\n","model=model.Sequential()\n","model.add(layers.Dense(32,activation='relu',input_shape=(784,)))\n","model.add(layers.Dense(10,activation='softmax'))\n","\n","#함수형 API 이용\n","input_tensor=layers.Input(shape=(784,))\n","x=layers.Dense(32,activation='relu')(input_tensor)\n","output_tensor=layers.Dense(10,activation='softmax')(x)\n","\n","model=models.Model(inputs=input_tensor,outputs=output_tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lPMj5h4DLitA","colab_type":"text"},"source":["**영화 리뷰 분류해보기: 이진 분류**\n","\n","본 실습은 imdb 데이터를 이용하여 영화평이 긍정인지 부정인지 분류하는 딥러닝 모델을 만드는 것을 실습한다.\n","\n","\n","신경망에 숫자 리스트를 그대로 주입할 수 없다.<br>\n","리스트를 텐서로 바꾸어 주어야 하는데 크게 2가지 방식이 있다.\n","\n","1. 같은 길이가 되도록 리스트에 패딩(padding)을 추가하고 (samples,sequence_length)크기의 정수 텐서로 변환한다. 그 다음 이 정수 텐서를 embedding층으로 사용한다.\n","\n","2. 리스트를 One-hot Encoding하여 0과 1의 벡터로 변환한다.\n","\n","**왜 활성화 함수를 쓰는가?**<br>\n","층을 깊게 만드는 장점을 살리기 위해서는 비선형성 또는 활성화 함수를 추가해야 한다. 주로 relu를 많이 사용한다.\n"]},{"cell_type":"code","metadata":{"id":"pahbXdHULr1R","colab_type":"code","colab":{}},"source":["from keras.datasets import imdb\n","from keras import models\n","from keras import layers\n","import numpy as np\n","\n","(train_data,train_labels),(test_data,test_labels)=imdb.load_data(num_words=10000) #데이터 로딩 + 자주 나타나는 단어 1만개만 사용하겠다. => 적절한 크기의 벡터 데이터를 얻기 위함.\n","\n","def make_one_hot(sequences,dimension=10000):\n","  results=np.zeros((len(sequences),dimension))\n","\n","  for i,sequence in enumerate(sequences):\n","    results[i,sequence]=1. #one-hot encoding\n","  return results\n","\n","\n","#전처리 \n","x_train=make_one_hot(train_data)\n","x_test=make_one_hot(test_data)\n","\n","y_train=np.asarray(train_labels).astype('float32')\n","y_test=np.asarray(test_labels).astype('float32')\n","\n","#모델 정의\n","model=models.Sequential()\n","model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))\n","model.add(layers.Dense(16,activation='relu'))\n","model.add(layers.Dense(1,activation='sigmoid'))\n","\n","#크로스앤트로피는 정보이론에서 온 개념으로 확률 분포간의 차이를 측정한다.(이진 분류에서 사용한다)\n","model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n","\n","#또 다른 방법\n","from keras import optimizers\n","model.compile(optimizer=optimizers.RMSprop(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","\n","#또 다른 방법\n","from keras import losses\n","from keras import metrics\n","model.compile(optimizer=optimizers.RMSprop(lr=0.001),loss=losses.binary_crossentropy,metrics=[metrics.binary_accuracy])\n","\n","#검증 데이터 만들기\n","x_val=x_train[:10000]\n","partial_x_train=x_train[10000:]\n","\n","y_val=y_train[:10000]\n","partial_y_train=y_train[10000:]\n","\n","#훈련하기\n","#model.fit이 History객체를 반환한다.\n","#객체는 (acc,loss,val_acc,val_loss))를 딕셔너리 형태로 가지고 있다.\n","#검증 데이터는 validation_data에 전달한다.\n","history=model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))\n","\n","#시각화 하기 - 훈련과 검증 손실\n","import matplotlib.pyplot as plt\n","\n","history_dict=history.history\n","loss=history_dict['loss']\n","val_loss=history_dict['val_loss']\n","\n","epochs=range(1,len(loss)+1)\n","\n","plt.plot(epochs,loss,'bo',label='Training loss')\n","plt.plot(epochs,val_loss,'b',label='Validatin_loss')\n","\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","#성능 측정\n","results=model.evaluate(x_test,y_test)\n","print(results)\n","\n","#훈련된 모델로 새로운 데이터에 대해서 예측해보기\n","model.predict(x_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SqywKMmFTtY5","colab_type":"text"},"source":["**뉴스기사 분류해보기: 다중 분류 문제**\n","\n","이번 실습은 단일 레이블 다중 분류 문제이다. <br>\n","로이터 데이터셋을 사용하여 46개의 토픽으로 분류해볼 예정이다.<br>"]},{"cell_type":"code","metadata":{"id":"hxNGnBnZNXRW","colab_type":"code","colab":{}},"source":["from keras.datasets import reuters\n","from keras.utils.np_utils import to_categorical\n","from keras import models\n","from keras import layers\n","import numpy as np\n","\n","(train_data,train_labels),(test_data,test_labels)=reuters.load_data(num_words=10000) #데이터 로딩 + 자주 나타나는 단어 1만개만 사용하겠다. => 적절한 크기의 벡터 데이터를 얻기 위함.\n","\n","\n","def make_one_hot(sequences,dimension=10000):\n","  results=np.zeros((len(sequences),dimension))\n","\n","  for i,sequence in enumerate(sequences):\n","    results[i,sequence]=1. #one-hot encoding\n","  return results\n","\n","\n","#전처리 \n","x_train=make_one_hot(train_data)\n","x_test=make_one_hot(test_data)\n","\n","y_train=to_categorical(train_labels)#원핫 인코딩 하기 (범주형 인코딩)\n","y_test=to_categorical(test_labels) #원핫 인코딩 하기 (범주형 인코딩)\n","\n","#모델 구성\n","model=models.Sequential()\n","model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\n","model.add(layers.Dense(64,activation='relu'))\n","model.add(layers.Dense(46,activation='softmax')) #46개로 분류하는 것이기에 !!!!!!!\n","\n","#컴파일\n","model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy']) #categorical_crossentropy는 두 확률 분포 사이의 거리를 측정한다.\n","\n","#검증 데이터 만들기\n","x_val=x_train[:1000]\n","partial_x_train=x_train[1000:]\n","\n","y_val=y_train[:1000]\n","partial_y_train=y_train[1000:]\n","\n","#훈련하기 \n","history=model.fit(partial_x_train,partial_y_train, epochs=20, batch_size=512, validation_data=(x_val,y_val))\n","\n","\n","#시각화 하기 - 훈련과 검증 손실\n","import matplotlib.pyplot as plt\n","\n","history_dict=history.history\n","loss=history_dict['loss']\n","val_loss=history_dict['val_loss']\n","\n","epochs=range(1,len(loss)+1)\n","\n","plt.plot(epochs,loss,'bo',label='Training loss')\n","plt.plot(epochs,val_loss,'b',label='Validatin_loss')\n","\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","\n","plt.legend()\n","\n","plt.show()\n","\n","#성능 측정\n","results=model.evaluate(x_test,y_test)\n","print(\"성능:\",results)\n","\n","#훈련된 모델로 새로운 데이터에 대해서 예측해보기\n","predictions=model.predict(x_test)\n","print(\"Predictions Shape: \",predictions[0].shape)\n","print(\"Sum:\", np.sum(predictions[0]))\n","print(\"Argmax: \",np.argmax(predictions[0]))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ocIWJk9JXq7f","colab_type":"text"},"source":["**회귀 문제: 주택 가격 예측 그리고 K-fold Cross Validation**<br>\n","\n","회귀와 로지스틱 회귀를 혼동하지 말것. <br>\n","로지스틱 회귀는 분류 알고리즘이다!!!! <br>\n","\n","개별적인 레이블 대신에 연속적인 !! 값을 예측하는 것이 회귀이다. <br>\n","\n","K-겹 교차 검증 (K-fold Cross validation)\n","데이터를 K개의 분할로 나누고 K개의 모델을 만들어 K-1개의 분할에서 훈련하고 나머지 분할에서 평가하는 방법\n","모델의 검증 점수는 K개의 검증 점수 평균이 된다."]},{"cell_type":"code","metadata":{"id":"QI_YO6pXYbTA","colab_type":"code","colab":{}},"source":["from keras.datasets import boston_housing\n","from keras.utils.np_utils import to_categorical\n","from keras import models\n","from keras import layers\n","import numpy as np\n","\n","(train_data,train_targets),(test_data,test_targets)=boston_housing.load_data() #데이터 로딩 + 자주 나타나는 단어 1만개만 사용하겠다. => 적절한 크기의 벡터 데이터를 얻기 위함.\n","print(\"학습 데이터 shape: \",train_data.shape)\n","print(\"테스트 데이터 shape: \",test_data.shape)\n","\n","\n","#정규화!!!!\n","#상이한 스케일을 가진 값을 신경망에 넣어주면 문제가 된다.\n","#따라서 정규화를 해주어야 한다.\n","#입력데이터에 있는 각 특성에 대해서 특성의 평균을 빼고 표준편차로 나누어 준다\n","#이렇게 되면 특성의 중앙이 0 근처에 맞추어 지고 표준편차가 1이 된다.\n","mean=train_data.mean(axis=0)\n","train_data-=mean\n","std=train_data.std(axis=0)\n","train_data/=std\n","\n","test_data-=mean\n","test_data/=std\n","\n","#모델 구성\n","#마지막 층에 활성화 함수가 없다. 이것이 전형적인 스칼라 회귀를 위한 구성이다.\n","#mse는 평균 제곱 오차(mean squared error)의 약어로 예측과 타깃 사이의 거리의 제곱이다. 이는 회귀 문제에서 널리 사용되는 loss 함수이다.\n","#mae는 Mean Absolute Error의 약자로 MAE가 0.5이면 예측이 평균적으로 500달러 정도 차이가 난다는 뜻이다.\n","def build_model():\n","  model=models.Sequential()\n","  model.add(layers.Dense(64,activation='relu',input_shape=(train_data.shape[1],))) #train_data.shape[1] ==> 13\n","  model.add(layers.Dense(64,activation='relu'))\n","  model.add(layers.Dense(1))\n","  model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\n","  return model\n","\n","#K-겹 교차 검증 (K-fold Cross validation)\n","#데이터를 K개의 분할로 나누고 K개의 모델을 만들어 K-1개의 분할에서 훈련하고 나머지 분할에서 평가하는 방법\n","#모델의 검증 점수는 K개의 검증 점수 평균이 된다.\n","k=4\n","num_val_samples=len(train_data)//k #k만큼 학습데이터 분할\n","num_epochs=100\n","\n","all_scores=[]\n","\n","for i in range(k):\n","  print(\"처리중인  fold\",i)\n","  val_data = train_data[i * num_val_samples : (i+1) * num_val_samples] #검증 데이터\n","  val_targets=train_targets[i * num_val_samples : (i+1) * num_val_samples] #검증 데이터ㅏ\n","\n","  partial_train_data=np.concatenate([train_data[:i*num_val_samples],\n","                                     train_data[(i+1)*num_val_samples:]]\n","                                    ,axis=0) #학습데이터 concat\n","  partial_train_targets=np.concatenate([train_targets[:i*num_val_samples],\n","                                     train_targets[(i+1)*num_val_samples:]]\n","                                    ,axis=0) #학습데이터 concat\n","  \n","  model=build_model()\n","  model.fit(partial_train_data,partial_train_targets,epochs=num_epochs,batch_size=1,verbose=0) #verbose=0이면 훈련과정을 출력하지 않는다.\n","\n","  val_mse,val_mae=model.evaluate(val_data,val_targets,verbose=0)\n","  all_scores.append(val_mae)\n","\n","print(\"전체 점수: \",all_scores)\n","print(\"평균: \",np.mean(all_scores))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmllLj5XffFj","colab_type":"text"},"source":["**머신러닝 깊게 알아보기**<br>\n","\n","**지도학습(Supervised Learning)**: 현재까지 실습한 예제 코드 모두 지도학습의 예들이다. 샘플 데이터가 주어지면 알고있는 타깃에 입력 데이터를 매핑하는 방법을 학습하는 것!\n","\n","**비지도학습(Unsupervised Learning)**: 어떤 타깃도 사용하지 않고 입력 데이터에 대한 변환을 찾아낸다. 차원 축소(Dimensionality reduction)와 군집(Clustering)이 비지도 학습의 예이다.\n","\n","**자기지도학습(Self Supervised Learning)**: 지도학습이나 사람이 만든 레이블을 사용하지 않는 것이 특징이다. 즉 학습과정에 사람이 개입하지 않는 지도학습. 레이블이 여전히 필요하지만 보통 Heuristic한 알고리즘을 사용해서 입력데이터로부터 생성한다.\n","대표적인 예시로 오토인코더(Autoencoder)가 있다.\n","\n","**강화학습(Reinforcement Learning)**: 구글 딥마인드의 알파고에 사용된 기법. 에이전트(Agent)는 환경에 대한 정보를 최대화하는 행동을 선택하도록 학습된다. \n","<br>\n","<br>\n","**머신러닝의 목표**: 처음 본 데이터에서 잘 작동하는 일반화 된 모델을 얻는 것.\n","<br>\n","\n","**학습데이터, 검증데이터, 테스트 데이터**ㅣ\n","학습 데이터로 모델을 훈련하고 검증 데이터로 모델을 평가한다. 모델을 출시할 준비가 되면 테스트 데이터에서 최종적으로 모델을 테스트 한다.\n","\n","**어떻게 나누어줄까?** \n","1. 단순 홀드 아웃 검증(Hold-out Validation)\n","2. K-겹 교차 검증(K-fold-Cross-Validation)\n","3. 셔플링(Shuffle)을 이용한 반복 K-겹 교차 검증 => K-겹 교차 검증을 여러 번 적용하되 K개의 분할로 나누기 전에 매번 데이터를 무작위로 썪는 것 !\n","<br>\n","<br>\n","\n","**신경망을 위한 데이터 전처리**\n","1. 벡터화: 신경망에서 모든 입력과 타깃은 부동 소수 데이터로 이루어진 텐서여야 한다.\n","2. 값 정규화: 작은 값을 취할 수록 좋다. 일반적으로 대부분의 값이 0~1사이여야 한다. 또한 균일해야 한다. 즉 모든 특성이 대체로 비슷한 범위를 가져야 한다.\n","\n","**과대 적합과 과소 적합**\n","1. 최적화: 가능한 훈련 데이터에서 최고의 성능을 얻으려고 모델을 조정하는 과정\n","2. 일반화: 훈련된 모델이 이전에 본적 없는 데이터에서 얼마나 잘 수행 되는지\n","\n","![대체 텍스트](https://image.slidesharecdn.com/2-170725041219/95/2supervised-learning-7-638.jpg?cb=1500972558)\n","\n","**어떻게 완화시킬까?** => 가중치 규제 추가(Weight Regularization) <br>\n","L1 규제: 가중치의 절대값에 비례하는 비용이 추가 (L1 No rm) <br>\n","L2 규제: 가중치의 제곱에 비례하는 비용이 추가 (L2 Norm) Weight Decay라고도 부름. <br>\n","\n","keras 예시 L1 규제<br>\n","model.add(layers.Dense(16,kernel_regularizer.l1(0.001),activation='relu',input_shape=(10000,)))<br>\n","\n","keras 예시 L2 규제<br>\n","model.add(layers.Dense(16,kernel_regularizer.l2(0.001),activation='relu',input_shape=(10000,)))\n","\n","\n","**어떻게 완화시킬까?**  ==> 드롭아웃\n","<br>\n","네트워크 층에 드롭아웃을 적용하면 훈련하는 동안 무작위로 층의 일부 출력 특성을 제외시킴.\n","<br>\n","keras 예시<br>\n","model.add(layers.Dropout(0.5))\n","\n","**머신러닝의 흐름**\n","![대체 텍스트](http://www.itworld.co.kr/sites/default/files/image/2016/10/machine-learning-pipeline-100723784-orig.jpg)\n"]}]}