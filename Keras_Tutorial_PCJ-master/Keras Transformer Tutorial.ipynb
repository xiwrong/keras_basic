{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Keras Transformer Tutorial.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9BQrQg8rFaU5","colab_type":"text"},"source":["**제목**: Keras Transformer Tutorial<br>\n","**제작자**: Park Chanjun (박찬준)<br>\n","**소속**: Korea University Natural Language Processing & Artificial Intelligence Lab (고려대학교 자연언어처리&인공지능 연구실)<br>\n","**Email**: bcj1210@naver.com<br>"]},{"cell_type":"markdown","metadata":{"id":"hG0El7TwFjnt","colab_type":"text"},"source":["**Install keras-transformer**"]},{"cell_type":"code","metadata":{"id":"EufxtQh3ErCP","colab_type":"code","colab":{}},"source":["!pip install keras-transformer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rYOzUEVbFogl","colab_type":"text"},"source":["**Train Transformer Model**"]},{"cell_type":"code","metadata":{"id":"tx0AnKd-EuLI","colab_type":"code","colab":{}},"source":["import numpy as np\n","from keras_transformer import get_model\n","\n","#예시 문장\n","tokens = '안녕하세요 저의 이름은 박찬준입니다. 만나서 반갑습니다. 저는 자연언어처리를 전공으로 하고 있습니다.'.split(' ')\n","\n","#토큰 딕셔너리 생성\n","token_dict = {\n","    '<PAD>': 0,\n","    '<START>': 1,\n","    '<END>': 2,\n","}\n","\n","#예시문장 토큰화 및 딕셔너리화\n","for token in tokens:\n","    if token not in token_dict:\n","        token_dict[token] = len(token_dict)\n","\n","#데이터 전처리 작업 (패딩 등)\n","encoder_inputs_no_padding = []\n","encoder_inputs, decoder_inputs, decoder_outputs = [], [], []\n","\n","for i in range(1, len(tokens) - 1):\n","    encode_tokens, decode_tokens = tokens[:i], tokens[i:]\n","    encode_tokens = ['<START>'] + encode_tokens + ['<END>'] + ['<PAD>'] * (len(tokens) - len(encode_tokens)) #패딩\n","    \n","    output_tokens = decode_tokens + ['<END>', '<PAD>'] + ['<PAD>'] * (len(tokens) - len(decode_tokens))\n","    \n","    decode_tokens = ['<START>'] + decode_tokens + ['<END>'] + ['<PAD>'] * (len(tokens) - len(decode_tokens))#패딩\n","    \n","    encode_tokens = list(map(lambda x: token_dict[x], encode_tokens))\n","    decode_tokens = list(map(lambda x: token_dict[x], decode_tokens))\n","    output_tokens = list(map(lambda x: [token_dict[x]], output_tokens))\n","    \n","    encoder_inputs_no_padding.append(encode_tokens[:i + 2])\n","    encoder_inputs.append(encode_tokens)\n","    \n","    decoder_inputs.append(decode_tokens)\n","    decoder_outputs.append(output_tokens)\n","\n","# 모델 생성 (keras_transformer 이용)\n","model = get_model(\n","    token_num=len(token_dict),\n","    embed_dim=30,\n","    encoder_num=3,\n","    decoder_num=2,\n","    head_num=3,\n","    hidden_dim=120,\n","    attention_activation='relu',\n","    feed_forward_activation='relu',\n","    dropout_rate=0.05,\n","    embed_weights=np.random.random((14, 30)),\n",")\n","\n","#모델 컴파일\n","model.compile(\n","    optimizer='adam',\n","    loss='sparse_categorical_crossentropy',\n",")\n","\n","#모델 써머리\n","model.summary()\n","\n","# 모델  훈련\n","model.fit(\n","    x=[np.asarray(encoder_inputs * 1000), np.asarray(decoder_inputs * 1000)],\n","    y=np.asarray(decoder_outputs * 1000),\n","    epochs=5,\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GsjHc7FvGXGP","colab_type":"text"},"source":["**Translation**"]},{"cell_type":"code","metadata":{"id":"MYzOveCTExlX","colab_type":"code","colab":{}},"source":["import numpy as np\n","from keras_transformer import get_model, decode\n","\n","#소스 문장\n","source_tokens = [\n","    '안녕하세요 저의 이름은 박찬준입니다.'.split(' '),\n","    '저는 24살입니다.'.split(' '),\n","]\n","\n","#타겟 문장\n","target_tokens = [\n","    list('Hello My name is Park Chanjun.'),\n","    list('I am 24 years old.'),\n","]\n","\n","#토큰 딕셔너리화 함수\n","def build_token_dict(token_list):\n","    token_dict = {\n","        '<PAD>': 0,\n","        '<START>': 1,\n","        '<END>': 2,\n","    }\n","    for tokens in token_list:\n","        for token in tokens:\n","            if token not in token_dict:\n","                token_dict[token] = len(token_dict)\n","    return token_dict\n","\n","\n","source_token_dict = build_token_dict(source_tokens) #딕셔너리화\n","target_token_dict = build_token_dict(target_tokens) #딕셔너리화\n","target_token_dict_inv = {v: k for k, v in target_token_dict.items()} #역으로.\n","\n","# <START>,<END>와 같은 Special Token 추가\n","encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n","decode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n","output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens]\n","\n","# 패딩\n","source_max_len = max(map(len, encode_tokens))\n","target_max_len = max(map(len, decode_tokens))\n","\n","encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n","decode_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in decode_tokens]\n","output_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in output_tokens]\n","\n","encode_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encode_tokens]\n","decode_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decode_tokens]\n","decode_output = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]\n","\n","\n","#모델 생성\n","model = get_model(\n","    token_num=max(len(source_token_dict), len(target_token_dict)),\n","    embed_dim=32,\n","    encoder_num=2,\n","    decoder_num=2,\n","    head_num=4,\n","    hidden_dim=128,\n","    dropout_rate=0.05,\n","    use_same_embed=False,  # Use different embeddings for different languages\n",")\n","\n","#모델 컴파일\n","model.compile('adam', 'sparse_categorical_crossentropy')\n","\n","#모델 써머리 \n","model.summary()\n","\n","#모델 훈련\n","model.fit(\n","    x=[np.array(encode_input * 1024), np.array(decode_input * 1024)],\n","    y=np.array(decode_output * 1024),\n","    epochs=10,\n","    batch_size=32,\n",")\n","\n","# 번역 진행 (Predict)\n","decoded = decode(\n","    model,\n","    encode_input,\n","    start_token=target_token_dict['<START>'],\n","    end_token=target_token_dict['<END>'],\n","    pad_token=target_token_dict['<PAD>'],\n",")\n","\n","print(''.join(map(lambda x: target_token_dict_inv[x], decoded[0][1:-1])))\n","print(''.join(map(lambda x: target_token_dict_inv[x], decoded[1][1:-1])))"],"execution_count":0,"outputs":[]}]}