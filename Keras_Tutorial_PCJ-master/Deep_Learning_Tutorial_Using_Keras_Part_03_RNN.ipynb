{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Tutorial Using Keras Part 03 - RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMDPT3aUihg2",
        "colab_type": "text"
      },
      "source": [
        "**제목**: Deep Learning Tutorial Using Keras Part03 - RNN (Keras로 배우는 딥러닝 튜토리얼 Part03 RNN편)<br>\n",
        "**제작자**: Park Chanjun (박찬준)<br>\n",
        "**소속**: Korea University Natural Language Processing & Artificial Intelligence Lab (고려대학교 자연언어처리&인공지능 연구실)<br>\n",
        "**Email**: bcj1210@naver.com<br>\n",
        "**참고자료**: 케라스 창시자에게 배우는 딥러닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EqNwJmJUHKe",
        "colab_type": "text"
      },
      "source": [
        "**텍스트와 시퀀스 데이터를 위한 딥러닝(자연언어처리)**\n",
        "<br>\n",
        "\n",
        "시퀀스 데이터를 처리하는 기본적인 딥러닝 모델은 <br>\n",
        "1. Recurrent Neural Network \n",
        "2. 1D Convnet\n",
        "\n",
        "\n",
        "크게 2가지가 존재한다.\n",
        "\n",
        "딥러닝 모델은 수치형 텐서만 다룰 수 있다.<br>\n",
        "따라서 텍스트를 수치형 텐서로 변환해야 한다.<br>\n",
        "이 과정을 텍스트 벡터화(Vectorizing)이라고 한다.<br>\n",
        "텍스트를 나누는 단위에(Token) 따라 크게 3가지 방식으로 이루어진다.\n",
        "\n",
        "1. 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환한다.\n",
        "2. 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환한다.\n",
        "3. 텍스트에서 단어나 문자의 n-gram을 추출하여 n-gram을 하나의 벡터로 변환한다.\n",
        "n-gram이란 문장에서 추출한 N개의 연속된 단어 그룹을 의미한다.\n",
        "\n",
        "Token과 Vector를 연결하는 2가지 방법\n",
        "1. One Hot Encoding\n",
        "2. (Word or Char or Sentence or Document) Embedding<br>\n",
        "a. 문제와 함께 단어 임베딩을 학습, 랜덤한 단어 벡터로 시작하여 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습<br>\n",
        "b. 사전 훈련된 단어 임베딩(Pretrain word Embedding)사용<br>\n",
        "\n",
        "![대체 텍스트](https://miro.medium.com/max/2560/1*52X2L01wpUjy39lIjofC7g.jpeg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn0LVT3Lig5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#One-Hot Encoding 실습 (단어 수준)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "samples=[\"안녕하세요 저의 이름은 박찬준입니다.\",\"지금부터 자연언어처리 강의를 시작하도록 하겠습니다.\"] #샘플 데이터\n",
        "\n",
        "token_index={}#토큰 딕셔너리\n",
        "\n",
        "for sample in samples:\n",
        "  for word in sample.split():#split을 이용하여 token으로 분리\n",
        "    if word not in token_index:\n",
        "      token_index[word]=len(token_index)+1 #단어마다 고유한 인덱스 부여, 인덱스 0은 사용하지 않기에 +1 을 하였음.\n",
        "\n",
        "max_length=10\n",
        "\n",
        "results=np.zeros(shape=(len(samples),\n",
        "                        max_length,\n",
        "                        max(token_index.values())+1))#결과를 저장할 배열\n",
        "\n",
        "for i,sample in enumerate(samples):\n",
        "  for j,word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index=token_index.get(word)\n",
        "    results[i,j,index]=1.\n",
        "\n",
        "print(results)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86sDMCoYihWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#One-Hot Encoding 실습 (문자 수준)\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "samples=[\"안녕하세요 저의 이름은 박찬준입니다.\",\"지금부터 자연언어처리 강의를 시작하도록 하겠습니다.\"] #샘플 데이터\n",
        "characters=string.printable #출력가능한 ASCII 문자\n",
        "\n",
        "token_index=dict(zip(characters, range(1,len(characters)+1))) #char 딕셔너리 생성\n",
        "\n",
        "max_length=50\n",
        "\n",
        "results=np.zeros(len(samples),max_length,max(token_index.values()+1))\n",
        "\n",
        "for i,sample in enumerate(samples):\n",
        "  for i,character in enumerate(sample):\n",
        "    index=token_index.get(character)\n",
        "    results[i,j,index]=1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB5RVWjUZAtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#One-Hot Encoding 실습 (Keras  이용)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples=[\"안녕하세요 저의 이름은 박찬준입니다.\",\"안녕하세요 지금부터 자연언어처리 강의를 시작하도록 하겠습니다.\"] #샘플 데이터\n",
        "\n",
        "tokenizer=Tokenizer(num_words=1000)#빈도수가 높은 1000개의 단어만 선택.\n",
        "tokenizer.fit_on_texts(samples)#단어 인덱스 구축\n",
        "\n",
        "sequences=tokenizer.texts_to_sequences(samples)#문자열을 정수 인덱스 리스트로 변환\n",
        "print(\"sequences: \",sequences)\n",
        "\n",
        "one_hot_results=tokenizer.texts_to_matrix(samples,mode='binary')#직접 one hot 이진 벡터 표현을 얻을 수 있다.\n",
        "print(\"one_hot_results: \",one_hot_results)\n",
        "\n",
        "word_index=tokenizer.word_index #단어 인덱스\n",
        "print(\"word_index: \",word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIxQA0V_egbA",
        "colab_type": "text"
      },
      "source": [
        "embedding_layer=Embedding(1000,64) #가능한 토큰의 개수(단어 인덱스 최대값+1), 임베딩 차원\n",
        "1. Embedding 층은 간단히 생각해 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리이다.\n",
        "2. 단어 인덱스 => Embedding층 => 연관된 단어 벡터\n",
        "3. 배치에 있는 모든 시퀀스는 길이가 같아야 하므로 작은 길이의 시퀀스는 0으로 패딩된다.\n",
        "\n",
        "4. Embedding 층은 (samples,sequence_length) 인 2D 정수 텐서를 입력으로 받는다.\n",
        "5. Embedding 층은 (sampels,sequence_length,embedding_dimensionality)인 3D 실수형 텐서를 반환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXmBbxzaay8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word Embedding 실습  => 문제와 함께 단어 임베딩을 학습, 랜덤한 단어 벡터로 시작하여 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습\n",
        "#저차원의 실수벡터 (보통 256,512,1024차원 정도 사용)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten,Dense,Embedding\n",
        "\n",
        "max_features=10000 #특성으로 사용할 단어의 수\n",
        "maxlen=20  #사용할 텍스트 길이\n",
        "\n",
        "#데이터 로딩\n",
        "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=max_features)\n",
        "\n",
        "#패딩\n",
        "x_train=preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen) #패딩 , (sampels,maxlen)의 2D 텐서 반환\n",
        "x_test=preprocessing.sequence.pad_sequences(x_test,maxlen=maxlen) #패딩 , (sampels,maxlen)의 2D 텐서 반환\n",
        "\n",
        "#모델 생성\n",
        "model=Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen)) #출력은 (samples,maxlen,8)\n",
        "\n",
        "model.add(Flatten()) #(samples,maxlen*8)의 2D 텐서로 펼친다.\n",
        "\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)\n",
        "\n",
        "test_loss,test_acc=model.evaluate(x_test,y_test)\n",
        "print(\"Test_acc: \",test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gneD3SfCgv86",
        "colab_type": "text"
      },
      "source": [
        "먼저 http://mng.bz/0tIo 에서 IMDB 원본 데이터셋을 다운로드하고 압축을 해제합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaobvmHRdqqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "imdb_dir = './datasets/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "#훈련용 리뷰 하나를 문자열 하나로 만들어 훈련 데이터를 문자열의 리스트로 구성. 리뷰 레이블(긍정/부정)도 labels 리스트로 만듬.\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding='utf8')\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # 100개 단어 이후는 버립니다\n",
        "training_samples = 200  # 훈련 샘플은 200개입니다\n",
        "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
        "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('데이터 텐서의 크기:', data.shape)\n",
        "print('레이블 텐서의 크기:', labels.shape)\n",
        "\n",
        "# 데이터를 훈련 세트와 검증 세트로 분할합니다.\n",
        "# 샘플이 순서대로 있기 때문에 (부정 샘플이 모두 나온 후에 긍정 샘플이 옵니다) \n",
        "# 먼저 데이터를 섞습니다.\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices) #셔플링 \n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxJ-2NADhFws",
        "colab_type": "text"
      },
      "source": [
        "GloVe 단어 임베딩 내려받기\n",
        "https://nlp.stanford.edu/projects/glove 에서 2014년 영문 위키피디아를 사용해 사전에 계산된 임베딩을 내려받습니다. 이 파일의 이름은 glove.6B.zip이고 압축 파일 크기는 823MB입니다. 400,000만개의 단어(또는 단어가 아닌 토큰)에 대한 100차원의 임베딩 벡터를 포함하고 있습니다. datasets 폴더 아래에 파일 압축을 해제합니다.(이 저장소에는 이미 포함되어 있습니다)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PUp0Z_OhHGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_dir = './datasets/'\n",
        "\n",
        "#Glove 처리 \n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0] #단어\n",
        "    coefs = np.asarray(values[1:], dtype='float32') #벡터값\n",
        "    embeddings_index[word] = coefs #딕셔너리\n",
        "f.close()\n",
        "\n",
        "print('%s개의 단어 벡터를 찾았습니다.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "#Embedding층에 주입할 수 있는 임베딩 행렬을 만들어야 한다.\n",
        "#행렬의 크기는 (max_words,embedding_dim)이어야 한다.\n",
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # 임베딩 인덱스에 없는 단어는 모두 0이 됩니다.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "#모델 정의\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "#모델에 GloVe 임베딩 로드하기\n",
        "#Embedding 층은 하나의 가중치 행렬을 가집니다. 이 행렬은 2D 부동 소수 행렬이고 각 i번째 원소는 i번째 인덱스에 상응하는 단어 벡터입니다. 모델의 첫 번째 층인 Embedding 층에 준비된 GloVe 행렬을 로드\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False #추가적으로 Embedding 층을 동결합니다\n",
        "\n",
        "#모델 컴파일\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "#모델 훈련\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "#모델 저장\n",
        "model.save_weights('pre_trained_glove_model.h5')\n",
        "\n",
        "#테스트 \n",
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)\n",
        "\n",
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOMaXmdNjolK",
        "colab_type": "text"
      },
      "source": [
        "**RNN**\n",
        "\n",
        "![대체 텍스트](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)\n",
        "\n",
        "<br>\n",
        "RNN은 (batch_size, timesteps,input_features)인 2D 텐서로 인코딩 된 벡터의 시퀀스를 입력 받음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQSV4ip8mHf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN\n",
        "\n",
        "max_features = 10000  # 특성으로 사용할 단어의 수\n",
        "maxlen = 500  # 사용할 텍스트의 길이(가장 빈번한 max_features 개의 단어만 사용합니다)\n",
        "batch_size = 32\n",
        "\n",
        "#데이터 로딩\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "#패딩\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train 크기:', input_train.shape)\n",
        "print('input_test 크기:', input_test.shape)\n",
        "\n",
        "from keras.layers import Dense\n",
        "\n",
        "#모델 정의 \n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#모델 컴파일\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "#모델 훈련\n",
        "history = model.fit(input_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#시각화 \n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOXBqbPDmuGQ",
        "colab_type": "text"
      },
      "source": [
        "**LSTM과 GRU**\n",
        "<br>\n",
        "Simple RNN의 문제점은 Vanishing Gradient Problem 즉 기울기 소실 문제.<br>\n",
        "이 문제를 해결하기 위한 것이 LSTM,GRU이다<br>\n",
        "과거 정보를 나중에 다시 주입하여 기울기 소실 문제를 해결하려 하는 것 !\n",
        "\n",
        "![대체 텍스트](https://miro.medium.com/max/3032/1*yBXV9o5q7L_CvY7quJt3WQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a7dTZVvnZ7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN\n",
        "\n",
        "max_features = 10000  # 특성으로 사용할 단어의 수\n",
        "maxlen = 500  # 사용할 텍스트의 길이(가장 빈번한 max_features 개의 단어만 사용합니다)\n",
        "batch_size = 32\n",
        "\n",
        "#데이터 로딩\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "#패딩\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train 크기:', input_train.shape)\n",
        "print('input_test 크기:', input_test.shape)\n",
        "\n",
        "\n",
        "#모델 정의 \n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(LSTM(32)) #LSTM 사용 !\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(input_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#시각화 \n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ763FUaoHHm",
        "colab_type": "text"
      },
      "source": [
        "**RNN 고급 사용법**\n",
        "\n",
        "1. 순환 드롭 아웃: Recurrent Dropout: 순환층에 과대적합 방지\n",
        "2. 스태킹 순환 층: Stacking Recurrent Layer: 네트워크 표현 능력 증가시킴\n",
        "3. Bidirectional Recurrent Layer: 양방향"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b38DIHOnogA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "#데이터 받아오기 \n",
        "!mkdir ./datasets\n",
        "!mkdir ./datasets/jena_climate/\n",
        "!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "!unzip jena_climate_2009_2016.csv.zip\n",
        "!mv jena_climate_2009_2016.csv ./datasets/jena_climate/\n",
        "\n",
        "#데이터 로딩\n",
        "data_dir = './datasets/jena_climate/'\n",
        "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
        "\n",
        "f = open(fname)\n",
        "data = f.read()\n",
        "f.close()\n",
        "\n",
        "lines = data.split('\\n')\n",
        "header = lines[0].split(',')\n",
        "lines = lines[1:]\n",
        "\n",
        "print(header)\n",
        "print(len(lines))\n",
        "\n",
        "#데이터 파싱 \n",
        "import numpy as np\n",
        "\n",
        "float_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(',')[1:]]\n",
        "    float_data[i, :] = values\n",
        "\n",
        "#정규화\n",
        "mean = float_data[:200000].mean(axis=0)\n",
        "float_data -= mean\n",
        "std = float_data[:200000].std(axis=0)\n",
        "float_data /= std\n",
        "\n",
        "#제너레이터 생성 \n",
        "def generator(data, lookback, delay, min_index, max_index,\n",
        "              shuffle=False, batch_size=128, step=6):\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - 1\n",
        "    i = min_index + lookback\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(\n",
        "                min_index + lookback, max_index, size=batch_size)\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arange(i, min(i + batch_size, max_index))\n",
        "            i += len(rows)\n",
        "\n",
        "        samples = np.zeros((len(rows),\n",
        "                           lookback // step,\n",
        "                           data.shape[-1]))\n",
        "        targets = np.zeros((len(rows),))\n",
        "        for j, row in enumerate(rows):\n",
        "            indices = range(rows[j] - lookback, rows[j], step)\n",
        "            samples[j] = data[indices]\n",
        "            targets[j] = data[rows[j] + delay][1]\n",
        "        yield samples, targets\n",
        "\n",
        "lookback = 1440\n",
        "step = 6\n",
        "delay = 144\n",
        "batch_size = 128\n",
        "\n",
        "train_gen = generator(float_data,\n",
        "                      lookback=lookback,\n",
        "                      delay=delay,\n",
        "                      min_index=0,\n",
        "                      max_index=200000,\n",
        "                      shuffle=True,\n",
        "                      step=step, \n",
        "                      batch_size=batch_size)\n",
        "val_gen = generator(float_data,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=200001,\n",
        "                    max_index=300000,\n",
        "                    step=step,\n",
        "                    batch_size=batch_size)\n",
        "test_gen = generator(float_data,\n",
        "                     lookback=lookback,\n",
        "                     delay=delay,\n",
        "                     min_index=300001,\n",
        "                     max_index=None,\n",
        "                     step=step,\n",
        "                     batch_size=batch_size)\n",
        "\n",
        "# 전체 검증 세트를 순회하기 위해 val_gen에서 추출할 횟수\n",
        "val_steps = (300000 - 200001 - lookback) // batch_size\n",
        "\n",
        "# 전체 테스트 세트를 순회하기 위해 test_gen에서 추출할 횟수\n",
        "test_steps = (len(float_data) - 300001 - lookback) // batch_size\n",
        "\n",
        "\n",
        "def evaluate_naive_method():\n",
        "    batch_maes = []\n",
        "    for step in range(val_steps):\n",
        "        samples, targets = next(val_gen)\n",
        "        preds = samples[:, -1, 1]\n",
        "        mae = np.mean(np.abs(preds - targets))\n",
        "        batch_maes.append(mae)\n",
        "    print(np.mean(batch_maes))\n",
        "    \n",
        "evaluate_naive_method()\n",
        "\n",
        "\n",
        "#GRU 사용\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))#GRU\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#드롭아웃 적용해보기 \n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "                     dropout=0.2, #드롭아웃\n",
        "                     recurrent_dropout=0.2, #드롭아웃 \n",
        "                     input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n",
        "\n",
        "#스태킹 순환층\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "                     dropout=0.1,\n",
        "                     recurrent_dropout=0.5,\n",
        "                     return_sequences=True,#요렇게 설정하면됨 !!!!!!!!!!!1\n",
        "                     input_shape=(None, float_data.shape[-1])))\n",
        "\n",
        "model.add(layers.GRU(64, activation='relu',\n",
        "                     dropout=0.1, \n",
        "                     recurrent_dropout=0.5))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n",
        "\n",
        "\n",
        "\n",
        "#거꾸로 된 시퀀스를 가지고 훈련해보기 \n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "# 특성으로 사용할 단어의 수\n",
        "max_features = 10000\n",
        "# 사용할 텍스트의 길이(가장 빈번한 max_features 개의 단어만 사용합니다)\n",
        "maxlen = 500\n",
        "\n",
        "# 데이터 로드\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# 시퀀스를 뒤집습니다!!!!!!!!!!!!!!!!!!!!!1\n",
        "x_train = [x[::-1] for x in x_train]\n",
        "x_test = [x[::-1] for x in x_test]\n",
        "\n",
        "# 시퀀스에 패딩을 추가합니다\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128))\n",
        "model.add(layers.LSTM(32))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "\n",
        "#양방향 LSTM\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 32))\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))#Bidirectional!!!!!!!!!\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Bidirectional(\n",
        "    layers.GRU(32), input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSN9v9VbtNTB",
        "colab_type": "text"
      },
      "source": [
        "**CNN을 이용한 시퀀스 처리**\n",
        "<br>\n",
        "![대체 텍스트](http://i.imgur.com/JN72JHW.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slAcTC_cpS6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000  # 특성으로 사용할 단어의 수\n",
        "max_len = 500  # 사용할 텍스트의 길이(가장 빈번한 max_features 개의 단어만 사용합니다)\n",
        "\n",
        "#데이터 로딩\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "\n",
        "#패딩\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "print('x_train 크기:', x_train.shape)\n",
        "print('x_test 크기:', x_test.shape)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "#모델 정의 \n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "#모델 출력\n",
        "model.summary()\n",
        "\n",
        "#모델 컴파일\n",
        "model.compile(optimizer=RMSprop(lr=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "#모델 훈련\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "\n",
        "test_loss,test_acc=model.evaluate(x_test,y_test)\n",
        "print(\"Test_acc: \",test_acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhoKVV2fuWQc",
        "colab_type": "text"
      },
      "source": [
        "**CNN과 RNN 연결하여 긴 시퀀스 처리하기**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxUgu1sLuc7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "data_dir = './datasets/jena_climate/'\n",
        "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
        "\n",
        "f = open(fname)\n",
        "data = f.read()\n",
        "f.close()\n",
        "\n",
        "lines = data.split('\\n')\n",
        "header = lines[0].split(',')\n",
        "lines = lines[1:]\n",
        "\n",
        "float_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(',')[1:]]\n",
        "    float_data[i, :] = values\n",
        "    \n",
        "mean = float_data[:200000].mean(axis=0)\n",
        "float_data -= mean\n",
        "std = float_data[:200000].std(axis=0)\n",
        "float_data /= std\n",
        "\n",
        "def generator(data, lookback, delay, min_index, max_index,\n",
        "              shuffle=False, batch_size=128, step=6):\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - 1\n",
        "    i = min_index + lookback\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(\n",
        "                min_index + lookback, max_index, size=batch_size)\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arange(i, min(i + batch_size, max_index))\n",
        "            i += len(rows)\n",
        "\n",
        "        samples = np.zeros((len(rows),\n",
        "                           lookback // step,\n",
        "                           data.shape[-1]))\n",
        "        targets = np.zeros((len(rows),))\n",
        "        for j, row in enumerate(rows):\n",
        "            indices = range(rows[j] - lookback, rows[j], step)\n",
        "            samples[j] = data[indices]\n",
        "            targets[j] = data[rows[j] + delay][1]\n",
        "        yield samples, targets\n",
        "        \n",
        "lookback = 1440\n",
        "step = 6\n",
        "delay = 144\n",
        "batch_size = 128\n",
        "\n",
        "train_gen = generator(float_data,\n",
        "                      lookback=lookback,\n",
        "                      delay=delay,\n",
        "                      min_index=0,\n",
        "                      max_index=200000,\n",
        "                      shuffle=True,\n",
        "                      step=step, \n",
        "                      batch_size=batch_size)\n",
        "\n",
        "val_gen = generator(float_data,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=200001,\n",
        "                    max_index=300000,\n",
        "                    step=step,\n",
        "                    batch_size=batch_size)\n",
        "\n",
        "test_gen = generator(float_data,\n",
        "                     lookback=lookback,\n",
        "                     delay=delay,\n",
        "                     min_index=300001,\n",
        "                     max_index=None,\n",
        "                     step=step,\n",
        "                     batch_size=batch_size)\n",
        "\n",
        "# 전체 검증 세트를 순회하기 위해 val_gen에서 추출할 횟수\n",
        "val_steps = (300000 - 200001 - lookback) // batch_size\n",
        "\n",
        "# 전체 테스트 세트를 순회하기 위해 test_gen에서 추출할 횟수\n",
        "test_steps = (len(float_data) - 300001 - lookback) // batch_size\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "#모델 정의\n",
        "model = Sequential()\n",
        "\n",
        "#Conv 1D\n",
        "model.add(layers.Conv1D(32, 5, activation='relu',\n",
        "                        input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "\n",
        "#GRU\n",
        "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}